{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in the dataset...\n",
      "TSPAN6                                                                                                 0\n",
      "TNMD                                                                                                   0\n",
      "DPM1                                                                                                   0\n",
      "SCYL3                                                                                                  0\n",
      "C1orf112                                                                                               0\n",
      "                                                                                                      ..\n",
      "Bacteria;Actinobacteria;Actinobacteria;Actinomycetales;Mycobacteriaceae;Mycobacterium;arupense         0\n",
      "Bacteria;Proteobacteria;Gammaproteobacteria;Pasteurellales;Pasteurellaceae;Gallibacterium;genomosp.    0\n",
      "Bacteria;Actinobacteria;Actinobacteria;Actinomycetales;Mycobacteriaceae;Mycobacterium;celatum          0\n",
      "Bacteria;Spirochaetes;GN05;LF030                                                                       0\n",
      "Bacteria;Lentisphaerae;[Lentisphaeria];Z20;R4-45B                                                      0\n",
      "Length: 20312, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schoudhry/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Step 12: Train the model\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Step 13: Visualize the model architecture using plot_model\u001b[39;00m\n\u001b[1;32m     90\u001b[0m plot_model(model, to_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_layer_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "csv12 = '/Users/schoudhry/Desktop/IIT/Research/researchData/StableS12.csv'\n",
    "csv13 = '/Users/schoudhry/Desktop/IIT/Research/researchData/Stable13.csv'\n",
    "csv1 = '/Users/schoudhry/Desktop/IIT/Research/researchData/Stable1.csv'\n",
    "\n",
    "# Load gene expression (S12), bacterial abundance (S13), and tumor/no tumor labels (S1)\n",
    "data_s12 = pd.read_csv(csv12)\n",
    "data_s13 = pd.read_csv(csv13)\n",
    "data_s1 = pd.read_csv(csv1)\n",
    "\n",
    "# Step 2: Extract relevant columns for B01 to B26\n",
    "patients = [f'B{str(i).zfill(2)}' for i in range(1, 27)]  # B01 to B26\n",
    "\n",
    "genes_data = data_s12[['Unnamed: 0'] + patients].rename(columns={'Unnamed: 0': 'Gene'})  # Gene expression\n",
    "bacteria_data = data_s13[['Unnamed: 0'] + patients].rename(columns={'Unnamed: 0': 'Bacteria'})  # Bacterial abundance\n",
    "\n",
    "# Step 3: Load the CRC status from csv1 (using 'Description' to label tumor=1 and normal=0)\n",
    "crc_status = data_s1[['Patient_Blind_ID', 'Description']].query(\"Patient_Blind_ID in @patients\")\n",
    "crc_status['CRC_Status'] = crc_status['Description'].apply(lambda x: 1 if x == 'tumor' else 0)\n",
    "\n",
    "# Step 4: Merge gene, bacteria, and CRC status data for all patients (B01 to B26)\n",
    "merged_data = pd.concat([genes_data.set_index('Gene').T, bacteria_data.set_index('Bacteria').T], axis=1)\n",
    "\n",
    "# Step 5: Convert all columns to numeric, force errors to NaN\n",
    "merged_data = merged_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 6: Check for any remaining non-numeric or NaN values\n",
    "print(\"Checking for NaN values in the dataset...\")\n",
    "print(merged_data.isnull().sum())  # Check for NaN values\n",
    "\n",
    "# Handle missing values (fill NaNs with 0 or use a different strategy)\n",
    "merged_data.fillna(0, inplace=True)\n",
    "\n",
    "# Merge with CRC status\n",
    "merged_data['CRC_Status'] = crc_status.set_index('Patient_Blind_ID')['CRC_Status']\n",
    "\n",
    "# Step 7: Prepare the input features (X) and target (y)\n",
    "X = merged_data.drop(columns=['CRC_Status']).values  # Features: gene and bacterial data\n",
    "y = merged_data['CRC_Status'].values  # Target: CRC status (tumor or no tumor)\n",
    "\n",
    "# Step 8: Normalize the data (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 9: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 10: Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_train.shape[1],)),  # Input layer\n",
    "\n",
    "    # First Dense Layer\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),  # Batch normalization for stable learning\n",
    "    layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "\n",
    "    # Second Dense Layer\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Third Dense Layer\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    # Fourth Dense Layer\n",
    "    layers.Dense(32, activation='relu'),\n",
    "\n",
    "    # Output Layer (sigmoid for binary classification)\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 11: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 12: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 13: Visualize the model architecture using plot_model\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Step 14: Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Step 15: Plot training history (accuracy and loss)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 16: Display the saved model plot\n",
    "from IPython.display import Image\n",
    "Image('model_architecture.png')  # Show the saved model plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12337 12423  3600  3792  5691  3566  3874  4771 11808 12379 12372 13796\n",
      " 14211  1762  1183  1343  1391  1457  1178  2093  1157  2186  2022  1944\n",
      "  2567  2635  7647  2681  1200  1926  2216  1340  2343  1277  2840  1619\n",
      "  4571  5741  4044  6470  6645  1552  8567  2853]\n"
     ]
    }
   ],
   "source": [
    "print(data_s1['Patient_Blind_ID'].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
